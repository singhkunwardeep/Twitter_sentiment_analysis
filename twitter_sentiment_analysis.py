# -*- coding: utf-8 -*-
"""twitter sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KL_EsYMZ0F9XdJZhiLxcGLZhrPIwh9J1
"""

# installing kaggle library
! pip install kaggle

# configure path of kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""**Importing Twitter DataSet**"""

# API to fetch the dataset from kaggle
!kaggle datasets download kazanova/sentiment140

# extracting the csv file from zip file

from zipfile import ZipFile
dataset = '/content/sentiment140.zip'

# r - read only
with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

"""**Importing dependencies**"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

# printing stopwords in english -- words that don't usually carry important meaning for NLP tasks.
print(stopwords.words('english'))

"""**DATA PROCESSING**"""

# loading data from csv file to pandas datafame

twitter_data  = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1')

# checking the numbers of rows and columns

twitter_data.shape

twitter_data.head()
# as we can see, it's reading tweets as column names, so we have to change that

column_names = ['target', 'id', 'date', 'flag', 'user', 'text']
twitter_data  = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', names = column_names,  encoding = 'ISO-8859-1')
twitter_data.head()

# checking if values are missing
twitter_data.isnull().sum()

# checking the distribution of target column
twitter_data['target'].value_counts()

# converting target 4 to 1
twitter_data.replace({'target': {4:1}}, inplace = True)
twitter_data['target'].value_counts()

# stemming -> process of reducing word to its root form


port_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)

  return stemmed_content

twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)

twitter_data.head()

# separating the data and label -> like we don't need date, user to find if the tweet is positive or negaitve

X = twitter_data['stemmed_content'].values
Y = twitter_data['target'].values

print(X)

"""**splitting dataset into training data and test data**"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y, random_state = 2)

"""**fit_transform() do?**

This does 2 things:

Learn the vocabulary from X_train (all the words and how often they appear)

Convert X_trainâ€™s text into numbers based on TF-IDF values

âš ï¸ Important:
We use fit_transform for training data because:

It needs to learn what words are there

And then transform them into numbers

 **transform() do?**

 Now for the test data â€” you donâ€™t want to learn new words. You only want to use the same rules/vocabulary you learned from training data and convert test data using that.
"""

# converting text data into numerical value - NLP feature extraction

vectorizer = TfidfVectorizer()

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

print(X_train)

"""TRAINING THE ML MODEL(logistic regression model)"""

# why ,max_iter = 1000 -> ðŸ‘‰ max_iter means "maximum number of iterations" the model is allowed to take while trying to
#find the best possible values for its internal parameters (also called weights or coefficients).
# When we convert tweets to numerical form (like using TF-IDF), each word becomes a number.
# Then, Logistic Regression assigns a weight (internal parameter) to each word feature.

model = LogisticRegression(max_iter=1000)

model.fit(X_train, Y_train)  # here model is learning from dataset

"""**model evaluation -> accuracy score**"""

# accuracy score on training data

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score( Y_train, X_train_prediction )

print('accuracy score on the training data : ', training_data_accuracy)

# accuracy score on test data

X_test_prediction = model.predict(X_test)
testing_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('accuracy score on the test data : ', testing_data_accuracy)

"""**saving trained model**"""

import pickle

filename = 'twitter_sentiment.sav'
pickle.dump(model, open(filename, 'wb'))